{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful application that is being explored for machine learning today is medical and healthcare-related problems. In this final project, I employed Python and machine learning algorithms from the scikit-learn library in order to make predictions of heart disease based on various markers from patient health records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained my data from the \"Heart Disease UCI\" database on Kaggle. \n",
    "\n",
    "The database contains the health information for 303 patients, some with heart disease and some without.  It also includes the values of 13 different markers of cardiovascular health for the patients.  My goal was to be able to train ML models on part of these datasets, and subsequently predict whether patients in the remaining data had heart disease or not based on the 13 markers of health. Such a model could help doctors predict heart disease in patients and support early detection of the disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the relevant packages and load the data using pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "data = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we can use a few convenient functions to explore it. \n",
    "\n",
    "First, using data.head(), we can peek at the contents of the data.  We can see that the first 13 columns contain various pieces of health-related data about each of the patients.  The 14th column contains a 0 or 1, a 0 if the patient has heart disease or a 1 if the patient does not. \n",
    "\n",
    "The 13 features in this dataset are:  \n",
    "- age \n",
    "- sex: (1 = male, 0 = female)\n",
    "- cp: Chest pain experienced (1: typical angina, 2: atypical angina, 3: non-anginal pain, Value 4: asymptomatic)\n",
    "- trestbps: Resting blood pressure, in mm Hg\n",
    "- chol: Cholesterol level, in mg/dl \n",
    "- fbs: Fasting blood sugar (Is > 120 mg/dl?, 1 = true; 0 = false)\n",
    "- restecg: Resting electrocardiographic measurement (0 = normal, 1 = ST-T wave abnormality, 2 = probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "- thalach: Maximum heart rate achieved\n",
    "- exang: Exercise induced angina (1 = yes; 0 = no)\n",
    "- oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot)\n",
    "- slope: Slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n",
    "- ca: Number of major vessels (0-3)\n",
    "- thal: Blood disorder called thalassemia (3 = normal, 6 = fixed defect, 7 = reversable defect) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using data.shape, we can obtain the shape of this data matrix.\n",
    "\n",
    "The dimensions turn out to be 303 x 14.  So, we have 303 rows and 14 columns.  The 303 rows indicate that there are 303 patients in the dataset, and the 14 columns represent the 13 features and 1 target (heart disease?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to divide the data into features and the target.  I accomplish this by storing the data columns containing the features to train the model on in the variable X, and storing the data column containing the target (the last column) in the variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('target', axis=1)  \n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to split the data into a training set to build the model, and a testing set to evaluate the model.  The scikit-learn package contains a useful library, train_test_split, which can be used to accomplish this splitting.  As shown below, I reserve 80% of the data for training purposes, and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have preprocessed the data and are ready to build and run machine learning models on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I used is Support Vector Machine, or SVM. SVM is a commonly used supervised machine learning classification algorithm.  \n",
    "\n",
    "To build an SVM model, I first import the SVC class from the scikit-learn package.  I am using the SVC (support vector classifier) class because I am performing a classification task.  The SVC task accepts one parameter, which is the kernel type. We set this parameter to \"linear\" because simple SVM's can only classify linearly separable data.(?) \n",
    "\n",
    "To train the algorithm on the training data, I call the fit method of the SVC class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the model using the training data, we can use the model to predict heart disease in patients in the testing data.  I do this by calling the predict method of the SVC class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can output a confusion matrix -- a table that is commonly used to evaluate the model's performance. \n",
    "\n",
    "In the confusion matrix, the first row represents the results for the class 0 (patients who do NOT have heart disease). The left value is the # of patients the model classified as 0 (not diseased), and the right value is the # of patients the model classified as 1 (diseased).  \n",
    "\n",
    "The second row represents the results for class 1 (patients who DO have heart disease). The left value is the # of patients the model classified as 0 (not diseased), and the right value is the # of patients the model classified as 1 (diseased). \n",
    "\n",
    "We can see that overall, the model did a decent job.  The testing set consists of 61 patients (20% of 303).  Out of the 26 patients without heart disease, the model classified 18 of them (69%) correctly as having no disease.  Out of the 35 patients with heart disease, the model classified all 34 of them (97.1%) correctly as having no disease. \n",
    "\n",
    "Ski-learn's metrics library also contains a function to output several formal metrics to evaluate the model: precision, recall, and f1-score.  \n",
    "\n",
    "What each of these metrics means: \n",
    "\n",
    "- *Precision*: Out of patients labeled as having/not having the disease, what percent of them actually had/didn't have the disease? \n",
    "\n",
    "This uses the left column for 0 and the right column for 1. \n",
    "\n",
    "For 0: 18/19, For 1: 34/42 \n",
    "\n",
    "- *Recall*: Out of patients with/without the disease, what percent of them were classified correctly? \n",
    "\n",
    "This uses the top row for 0 and the bottom row for 1. \n",
    "\n",
    "For 0: 18/26, For 1: 34/35 \n",
    "\n",
    "- *f1-score*:  Weighted average of the precision and recall.  F1-score = 2*(Recall*Precision) / (Recall + Precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  8]\n",
      " [ 1 34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80        26\n",
      "           1       0.81      0.97      0.88        35\n",
      "\n",
      "   micro avg       0.85      0.85      0.85        61\n",
      "   macro avg       0.88      0.83      0.84        61\n",
      "weighted avg       0.87      0.85      0.85        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model I used is Logistic Regression, another common machine learning algorithm that uses the sigmoid function to make predictions and for 0 or 1 classification.    \n",
    "\n",
    "To build a logistic regression model, I first import the LogisticRegression class from the scikit-learn library.  Then, \n",
    "I train the algorithm on the training data by calling the fit() method in the LogisticRegression class.  \n",
    "\n",
    "Finally, I use the model to make predictions on the testing set by calling the predict() method of the \n",
    "LogisticRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rohan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred2 =logreg.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for SVM, I outputted a confusion matrix to evaluate the performance of the Logistic Regression model. As a reminder, this is how each of the metrics was calculated:\n",
    "\n",
    "- Precision: Out of patients labeled as having/not having the disease, what percent of them actually had/didn't have the disease?\n",
    "\n",
    "This uses the left column for 0 and the right column for 1.\n",
    "\n",
    "For 0: 18/18, For 1: 35/43 \n",
    "\n",
    "- Recall: Out of patients with/without the disease, what percent of them were classified correctly?\n",
    "\n",
    "This uses the top row for 0 and the bottom row for 1.\n",
    "\n",
    "For 0: 18/26, For 1: 35/35  \n",
    "\n",
    "- f1-score: Weighted average of the precision and recall. F1-score = 2(RecallPrecision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  8]\n",
      " [ 0 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.82        26\n",
      "           1       0.81      1.00      0.90        35\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        61\n",
      "   macro avg       0.91      0.85      0.86        61\n",
      "weighted avg       0.89      0.87      0.86        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred2))  \n",
    "print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: XGBoost (Tree Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model I used was XGBoost, an algorithm that utilizes decision trees for predicting its target's values. XGBoost uses gradient boosting to improve the speed and performance of the decision tree algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I make predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the results:  \n",
    "\n",
    "- Precision: Out of patients labeled as having/not having the disease, what percent of them actually had/didn't have the disease?\n",
    "\n",
    "This uses the left column for 0 and the right column for 1.\n",
    "\n",
    "For 0: 20/25, For 1: 30/36 \n",
    "\n",
    "- Recall: Out of patients with/without the disease, what percent of them were classified correctly?\n",
    "\n",
    "This uses the top row for 0 and the bottom row for 1.\n",
    "\n",
    "For 0: 20/26, For 1: 30/335 \n",
    "\n",
    "- f1-score: Weighted average of the precision and recall. F1-score = 2(RecallPrecision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  6]\n",
      " [ 5 30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        26\n",
      "           1       0.83      0.86      0.85        35\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        61\n",
      "   macro avg       0.82      0.81      0.81        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred3))  \n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 algorithms I tried -- SVM, Logistic Regression, and XGBoost -- all performed well at predicting heart diseases from the 13 features in the dataset after being trained on 80% of the 303 total patient cases. \n",
    "\n",
    "Comparison between the algorithms' performances: \n",
    "- We can compare the f1-scores of the 2 models as the f1 scores are a good \"catch-all\" metric to evaluate the performance of a model.  \n",
    "- SVM had an f1-score of 80% for 0 and 88% for 1 \n",
    "- LogReg had an f1-score of 82% for 0 and 90% for 1 \n",
    "- XGBoost had an f1-score of 78% of 0 and 85% for 1 \n",
    "\n",
    "Overall, all 3 models had quite high f1-scores, indicating strong predictive ability.  Interestingly, all 3 models seemed to perform better in classifying patients who had the disease vs. patients who did not have the disease.  Finally, although in this run it seems that Logistic Regression performed better than SVM, which performed better than XGBoost, I can't read too much into these results.  This is because each time I ran the 3 algorithms, I got different rankings pretty much every time -- no model consistently outperformed the others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
